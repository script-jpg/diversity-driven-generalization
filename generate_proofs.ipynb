{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Folder Output Path\n",
        "Modify folder_path to your preference"
      ],
      "metadata": {
        "id": "3sV6REOCCbXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comment out this part if not using Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7f4Yaj0BumY",
        "outputId": "cff909eb-5029-4b15-9e4d-0f50c4229989"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faf81b7e",
        "outputId": "0ba40e0b-3e0d-4d56-e57b-794c3a49da5e"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the path to the new folder in Google Drive\n",
        "folder_path = '/content/drive/MyDrive/proofs_miniF2F' # set this to where you want the proofs saved\n",
        "\n",
        "# Create the folder if it doesn't exist\n",
        "if not os.path.exists(folder_path):\n",
        "    os.makedirs(folder_path)\n",
        "    print(f\"Folder '{folder_path}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Folder '{folder_path}' already exists.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder '/content/drive/MyDrive/proofs_miniF2F' created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTqIr3YPDHyu",
        "outputId": "c895912c-5ff3-42b4-a0a1-b147948dcf83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Goedel-LM/Goedel-Prover-SFT',\n",
              " 'ByteDance-Seed/BFS-Prover',\n",
              " 'AI-MO/Kimina-Prover-Preview-Distill-7B')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from itertools import combinations\n",
        "from typing import List, Tuple\n",
        "\n",
        "solver_model_ids = [\n",
        "    \"Goedel-LM/Goedel-Prover-SFT\",\n",
        "    \"ByteDance-Seed/BFS-Prover\",\n",
        "    \"AI-MO/Kimina-Prover-Preview-Distill-7B\",\n",
        "    \"deepseek-ai/DeepSeek-Prover-V2-7B\",\n",
        "    \"deepseek-ai/DeepSeek-Prover-V1.5-RL\",\n",
        "]\n",
        "\n",
        "def get_all_combinations_of_length(s: set, length: int) -> List[Tuple]:\n",
        "    return list(combinations(s, length))\n",
        "\n",
        "solver_model_ensembles_3 = get_all_combinations_of_length(solver_model_ids, 3)\n",
        "# assert(len(solver_model_ids) == 15)\n",
        "# assert(len(solver_model_ensembles_3) == 455)\n",
        "solver_model_ensembles_3[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rest of Experiment"
      ],
      "metadata": {
        "id": "9Un6YzYVCyX_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j52IY6kjcwgL",
        "outputId": "9cc98d1d-1f0a-40d0-86ed-46be1318c21b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.0\n",
            "    Uninstalling transformers-4.57.0:\n",
            "      Successfully uninstalled transformers-4.57.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ofvmly_7IbMs"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
        "import logging\n",
        "\n",
        "def _load_model(model_id):\n",
        "    \"\"\"\n",
        "    Loads a single model and tokenizer to the GPU, with a fix for rope_scaling issues.\n",
        "    \"\"\"\n",
        "    print(f\"Attempting to load model: {model_id}\")\n",
        "    try:\n",
        "        # 1. Load configuration first\n",
        "        config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "        # 3. Load model and tokenizer with the (potentially corrected) config\n",
        "        tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            config=config, # Pass the corrected config\n",
        "            torch_dtype=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        print(f\"Successfully loaded {model_id}\")\n",
        "        return model, tok\n",
        "\n",
        "    except Exception as e:\n",
        "        # Provide a more informative error message\n",
        "        logging.error(f\"❌ Failed to load model '{model_id}'. Error: {e}\")\n",
        "        # Return None to be handled by the calling function, preventing the crash\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def generate_proof(pipe, header, informal_prefix, formal_statement,\n",
        "                   temperature: float = 0.5, max_new_tokens: int = 4096,\n",
        "                   num_return_sequences: int = 1):\n",
        "    prompt = f\"{header}\\n\\n{informal_prefix}\\n{formal_statement}\\n\"\n",
        "\n",
        "    # Prepare arguments for the pipeline\n",
        "    generation_args = {\n",
        "        'do_sample': True,\n",
        "        'eos_token_id': pipe.tokenizer.eos_token_id,\n",
        "        'num_return_sequences': num_return_sequences\n",
        "    }\n",
        "\n",
        "    # Only add max_new_tokens if a value is provided\n",
        "    # If it remains None, the pipeline will use its own default\n",
        "    if max_new_tokens is not None:\n",
        "        generation_args['max_new_tokens'] = max_new_tokens\n",
        "\n",
        "    # Call the pipeline with the arguments\n",
        "    out = pipe(prompt, **generation_args)\n",
        "\n",
        "    proofs = [result['generated_text'][len(prompt):].strip() for result in out]\n",
        "    return proofs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVABURhGvtFe"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import gc\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline\n",
        "\n",
        "def _sanitize_dir_name(name: str) -> str:\n",
        "    return str(name).replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
        "\n",
        "def _next_index(out_dir: Path) -> int:\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    nums = []\n",
        "    for p in out_dir.glob(\"*.txt\"):\n",
        "        stem = p.stem\n",
        "        if stem.isdigit():\n",
        "            nums.append(int(stem))\n",
        "    return (max(nums) + 1) if nums else 1\n",
        "\n",
        "def generate_proofs_memory_safe(\n",
        "    model_ids,\n",
        "    problem_row,\n",
        "    problem_key,                 # e.g., DataFrame index or a unique ID column\n",
        "    max_attempts: int,\n",
        "    base_output_dir: str, # Added base_output_dir\n",
        "    gpu_batch_size: int = 8,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate proofs and write to base_output_dir/<problem_key>/<model_id>/1.txt, 2.txt, ...\n",
        "    No proof checking; purely generation + IO. Memory-safe (loads one model at a time).\n",
        "    \"\"\"\n",
        "    for model_id in tqdm(model_ids, desc=\"Models\"):\n",
        "        model = tok = pipe = None\n",
        "        attempt_bar = None\n",
        "        try:\n",
        "            model, tok = _load_model(model_id)\n",
        "            pipe = pipeline(\"text-generation\", model=model, tokenizer=tok, device=0)\n",
        "\n",
        "            attempts_left = max_attempts\n",
        "            attempt_bar = tqdm(total=max_attempts, desc=f\"Generating {model_id}\", leave=False)\n",
        "\n",
        "            # Prepare output directory and next index (continues numbering if rerun)\n",
        "            out_dir = Path(base_output_dir) / _sanitize_dir_name(problem_key) / _sanitize_dir_name(model_id) # Modified out_dir\n",
        "            next_idx = _next_index(out_dir)\n",
        "\n",
        "            print(next_idx)\n",
        "\n",
        "            # raise Exception(\"Stop here\")\n",
        "\n",
        "            while attempts_left > 0:\n",
        "                current_batch_size = min(gpu_batch_size, attempts_left)\n",
        "                with torch.no_grad():\n",
        "                    proof_snippets = generate_proof(\n",
        "                        pipe,\n",
        "                        header=problem_row['header'],\n",
        "                        informal_prefix=problem_row['informal_prefix'],\n",
        "                        formal_statement=problem_row['formal_statement'],\n",
        "                        num_return_sequences=current_batch_size\n",
        "                    )\n",
        "\n",
        "                # Write each snippet to numbered files 1.txt, 2.txt, ...\n",
        "                for snippet in proof_snippets:\n",
        "                    out_path = out_dir / f\"{next_idx}.txt\"\n",
        "                    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                        f.write(snippet)\n",
        "                    next_idx += 1\n",
        "\n",
        "                attempts_left -= current_batch_size\n",
        "                attempt_bar.update(current_batch_size)\n",
        "\n",
        "        finally:\n",
        "            if attempt_bar is not None:\n",
        "                attempt_bar.close()\n",
        "            if model: del model\n",
        "            if tok:   del tok\n",
        "            if pipe:  del pipe\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "def write_proofs_for_model(\n",
        "    model_id: str,\n",
        "    dataframe,\n",
        "    base_output_dir: str, # Added base_output_dir\n",
        "    max_attempts: int = 8,\n",
        "    gpu_batch_size: int = 8\n",
        "):\n",
        "    \"\"\"\n",
        "    For each problem in `dataframe`, generate `max_attempts` proofs for `model_id`\n",
        "    and write them to base_output_dir/<problem_key>/<model_id>/*.txt.\n",
        "    `problem_key` defaults to the DataFrame index value.\n",
        "    \"\"\"\n",
        "    print(f\"--- Generating proofs for model: {model_id} ---\")\n",
        "    for idx, problem_row in tqdm(dataframe.iterrows(), total=len(dataframe), desc=f\"Problems for {model_id}\"):\n",
        "        problem_key = problem_row.get('problem_id', idx)  # prefer a column named 'problem_id' if present\n",
        "        generate_proofs_memory_safe(\n",
        "            model_ids=[model_id],\n",
        "            problem_row=problem_row,\n",
        "            problem_key=problem_key,\n",
        "            max_attempts=max_attempts,\n",
        "            base_output_dir=base_output_dir, # Passed base_output_dir\n",
        "            gpu_batch_size=gpu_batch_size,\n",
        "        )\n",
        "    print(f\"--- Done for {model_id}. ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlT_gIjCWrtu"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga7ZkAA_v9Q7"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "# from google.colab import files\n",
        "# import time\n",
        "\n",
        "miniF2F_test_df = load_dataset(\"HaimingW/miniF2F-lean4\", split=\"test\").to_pandas().iloc[40:]\n",
        "\n",
        "for i, mid in enumerate(solver_model_ids):\n",
        "    try:\n",
        "      write_proofs_for_model(mid, miniF2F_test_df, base_output_dir=folder_path, max_attempts=8, gpu_batch_size=3) # Passed folder_path\n",
        "    except Exception as e:\n",
        "      print(f\"Error for {mid}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lk5CzQFvcwgO"
      },
      "outputs": [],
      "source": [
        "!zip -r proofs.zip proofs"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}