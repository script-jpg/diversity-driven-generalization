{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# -*- coding: utf-8 -*-\n", "\"\"\"generate_proofs.ipynb"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Automatically generated by Colab."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Original file is located at\n", "    https://colab.research.google.com/drive/1hZx9iXeY43XWp_7yysX2wACh5N5jcr7k"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Setup Folder Output Path"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Modify folder_path to your preference\n", "\"\"\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Comment out this part if not using Google drive"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from google.colab import drive\n", "drive.mount('/content/drive')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Define the path to the new folder in Google Drive"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["folder_path = '/content/drive/MyDrive/proofs_miniF2F' # set this to where you want the proofs saved"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create the folder if it doesn't exist"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if not os.path.exists(folder_path):\n", "    os.makedirs(folder_path)\n", "    print(f\"Folder '{folder_path}' created successfully.\")\n", "else:\n", "    print(f\"Folder '{folder_path}' already exists.\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from typing import List, Tuple"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["solver_model_ids = [\n", "    # \"Goedel-LM/Goedel-Prover-SFT\",\n", "    # \"AI-MO/Kimina-Prover-Preview-Distill-7B\",\n", "    \"deepseek-ai/DeepSeek-Prover-V2-7B\",\n", "    # \"deepseek-ai/DeepSeek-Prover-V1.5-RL\",\n", "]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n# Rest of Experiment\n<br>\n", "!pip install --upgrade transformers<br>\n", "!pip install tqdm<br>\n", "from transformers import pipeline, AutoConfig, AutoTokenizer, AutoModelForCausalLM<br>\n", "import logging<br>\n", "def _load_model(model_id):<br>\n", "  \n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    Loads a single model and tokenizer to the GPU, with a fix for rope_scaling issues.\n", "    \"\"\"\n", "    print(f\"Attempting to load model: {model_id}\")\n", "    try:\n", "        # 1. Load configuration first\n", "        config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n\n", "        # 3. Load model and tokenizer with the (potentially corrected) config\n", "        tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n", "        model = AutoModelForCausalLM.from_pretrained(\n", "            model_id,\n", "            config=config, # Pass the corrected config\n", "            torch_dtype=\"auto\",\n", "            trust_remote_code=True\n", "        ).to(\"cuda\")\n", "        print(f\"Successfully loaded {model_id}\")\n", "        return model, tok\n", "    except Exception as e:\n", "        # Provide a more informative error message\n", "        logging.error(f\"\u274c Failed to load model '{model_id}'. Error: {e}\")\n", "        # Return None to be handled by the calling function, preventing the crash\n", "        return None, None"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def generate_proof(pipe, header, informal_prefix, formal_statement,\n", "                   temperature: float = 0.5, max_new_tokens: int = 4096,\n", "                   num_return_sequences: int = 1):\n", "    prompt = f\"{header}\\n\\n{informal_prefix}\\n{formal_statement}\\n\"\n\n", "    # Prepare arguments for the pipeline\n", "    generation_args = {\n", "        'do_sample': True,\n", "        'eos_token_id': pipe.tokenizer.eos_token_id,\n", "        'num_return_sequences': num_return_sequences\n", "    }\n\n", "    # Only add max_new_tokens if a value is provided\n", "    # If it remains None, the pipeline will use its own default\n", "    if max_new_tokens is not None:\n", "        generation_args['max_new_tokens'] = max_new_tokens\n\n", "    # Call the pipeline with the arguments\n", "    out = pipe(prompt, **generation_args)\n", "    proofs = [result['generated_text'][len(prompt):].strip() for result in out]\n", "    return proofs"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pathlib import Path\n", "import gc\n", "import torch\n", "from tqdm import tqdm\n", "from transformers import pipeline"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def _sanitize_dir_name(name: str) -> str:\n", "    return str(name).replace(\"/\", \"_\").replace(\"\\\\\", \"_\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def _next_index(out_dir: Path) -> int:\n", "    out_dir.mkdir(parents=True, exist_ok=True)\n", "    nums = []\n", "    for p in out_dir.glob(\"*.txt\"):\n", "        stem = p.stem\n", "        if stem.isdigit():\n", "            nums.append(int(stem))\n", "    return (max(nums) + 1) if nums else 1"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model_ref = None\n", "tok_ref = None\n", "pipe_ref = None\n", "def generate_proofs_memory_safe(\n", "    model_ids,\n", "    problem_row,\n", "    problem_key,                 # e.g., DataFrame index or a unique ID column\n", "    max_attempts: int,\n", "    base_output_dir: str, # Added base_output_dir\n", "    gpu_batch_size: int = 8,\n", "    clear = True\n", "):\n", "    \"\"\"\n", "    Generate proofs and write to base_output_dir/<problem_key>/<model_id>/1.txt, 2.txt, ...\n", "    No proof checking; purely generation + IO. Memory-safe (loads one model at a time).\n", "    \"\"\"\n", "    for model_id in tqdm(model_ids, desc=\"Models\"):\n", "        model = tok = pipe = None\n", "        attempt_bar = None\n", "        try:\n", "            model, tok = _load_model(model_id)\n", "            pipe = pipeline(\"text-generation\", model=model, tokenizer=tok, device=0)\n", "            attempts_left = max_attempts\n", "            attempt_bar = tqdm(total=max_attempts, desc=f\"Generating {model_id}\", leave=False)\n\n", "            # Prepare output directory and next index (continues numbering if rerun)\n", "            out_dir = Path(base_output_dir) / _sanitize_dir_name(problem_key) / _sanitize_dir_name(model_id) # Modified out_dir\n", "            next_idx = _next_index(out_dir)\n", "            print(next_idx)\n\n", "            # raise Exception(\"Stop here\")\n", "            while attempts_left > 0:\n", "                current_batch_size = min(gpu_batch_size, attempts_left)\n", "                with torch.no_grad():\n", "                    proof_snippets = generate_proof(\n", "                        pipe,\n", "                        header=problem_row['header'],\n", "                        informal_prefix=problem_row['informal_prefix'],\n", "                        formal_statement=problem_row['formal_statement'],\n", "                        num_return_sequences=current_batch_size\n", "                    )\n", "                # Write each snippet to numbered files 1.txt, 2.txt, ...\n", "                for snippet in proof_snippets:\n", "                    out_path = out_dir / f\"{next_idx}.txt\"\n", "                    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n", "                        f.write(snippet)\n", "                    next_idx += 1\n", "                attempts_left -= current_batch_size\n", "                attempt_bar.update(current_batch_size)\n", "        finally:\n", "          model_ref = model\n", "          pipe_ref = pipe\n", "          tok_ref = tok        \n", "          # if clear:\n", "          #   if attempt_bar is not None:\n", "          #       attempt_bar.close()\n", "          #   if model: del model\n", "          #   if tok:   del tok\n", "          #   if pipe:  del pipe\n", "          #   gc.collect()\n", "          #   torch.cuda.empty_cache()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def write_proofs_for_model(\n", "    model_id: str,\n", "    dataframe,\n", "    base_output_dir: str, # Added base_output_dir\n", "    max_attempts: int = 8,\n", "    gpu_batch_size: int = 8,\n", "    clear = True\n", "):\n", "    \"\"\"\n", "    For each problem in `dataframe`, generate `max_attempts` proofs for `model_id`\n", "    and write them to base_output_dir/<problem_key>/<model_id>/*.txt.\n", "    `problem_key` defaults to the DataFrame index value.\n", "    \"\"\"\n", "    print(f\"--- Generating proofs for model: {model_id} ---\")\n", "    for idx, problem_row in tqdm(dataframe.iterrows(), total=len(dataframe), desc=f\"Problems for {model_id}\"):\n", "        problem_key = problem_row.get('problem_id', idx)  # prefer a column named 'problem_id' if present\n", "        generate_proofs_memory_safe(\n", "            model_ids=[model_id],\n", "            problem_row=problem_row,\n", "            problem_key=problem_key,\n", "            max_attempts=max_attempts,\n", "            base_output_dir=base_output_dir, # Passed base_output_dir\n", "            gpu_batch_size=gpu_batch_size,\n", "            clear = clear\n", "        )\n", "    if clear:\n", "        # if attempt_bar is not None:\n", "        #     attempt_bar.close()\n", "        if model_ref: del model_ref\n", "        if tok_ref:   del tok_ref\n", "        if pipe_ref:  del pipe_ref\n", "        gc.collect()\n", "        torch.cuda.empty_cache()\n", "    print(f\"--- Done for {model_id}. ---\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install sentencepiece"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from datasets import load_dataset\n", "# from google.colab import files\n", "# import time"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["miniF2F_test_df = load_dataset(\"HaimingW/miniF2F-lean4\", split=\"test\").to_pandas().iloc[9:40]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for i, mid in enumerate(solver_model_ids):\n", "    try:\n", "      write_proofs_for_model(mid, miniF2F_test_df, base_output_dir=folder_path, max_attempts=16, gpu_batch_size=4, clear=False) # Passed folder_path\n", "    except Exception as e:\n", "      print(f\"Error for {mid}: {e}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!zip -r proofs.zip proofs"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}