# -*- coding: utf-8 -*-
"""generate_proofs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hZx9iXeY43XWp_7yysX2wACh5N5jcr7k

# Setup Folder Output Path
Modify folder_path to your preference
"""

# Comment out this part if not using Google drive
from google.colab import drive
drive.mount('/content/drive')

import os

# Define the path to the new folder in Google Drive
folder_path = '/content/drive/MyDrive/proofs_miniF2F' # set this to where you want the proofs saved

# Create the folder if it doesn't exist
if not os.path.exists(folder_path):
    os.makedirs(folder_path)
    print(f"Folder '{folder_path}' created successfully.")
else:
    print(f"Folder '{folder_path}' already exists.")

from typing import List, Tuple

solver_model_ids = [
    # "Goedel-LM/Goedel-Prover-SFT",
    # "AI-MO/Kimina-Prover-Preview-Distill-7B",
    "deepseek-ai/DeepSeek-Prover-V2-7B",
    # "deepseek-ai/DeepSeek-Prover-V1.5-RL",
]

"""# Rest of Experiment"""

!pip install --upgrade transformers
!pip install tqdm

from transformers import pipeline, AutoConfig, AutoTokenizer, AutoModelForCausalLM
import logging

def _load_model(model_id):
    """
    Loads a single model and tokenizer to the GPU, with a fix for rope_scaling issues.
    """
    print(f"Attempting to load model: {model_id}")
    try:
        # 1. Load configuration first
        config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)

        # 3. Load model and tokenizer with the (potentially corrected) config
        tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            config=config, # Pass the corrected config
            torch_dtype="auto",
            trust_remote_code=True
        ).to("cuda")

        print(f"Successfully loaded {model_id}")
        return model, tok

    except Exception as e:
        # Provide a more informative error message
        logging.error(f"❌ Failed to load model '{model_id}'. Error: {e}")
        # Return None to be handled by the calling function, preventing the crash
        return None, None


def generate_proof(pipe, header, informal_prefix, formal_statement,
                   temperature: float = 0.5, max_new_tokens: int = 4096,
                   num_return_sequences: int = 1):
    prompt = f"{header}\n\n{informal_prefix}\n{formal_statement}\n"

    # Prepare arguments for the pipeline
    generation_args = {
        'do_sample': True,
        'eos_token_id': pipe.tokenizer.eos_token_id,
        'num_return_sequences': num_return_sequences
    }

    # Only add max_new_tokens if a value is provided
    # If it remains None, the pipeline will use its own default
    if max_new_tokens is not None:
        generation_args['max_new_tokens'] = max_new_tokens

    # Call the pipeline with the arguments
    out = pipe(prompt, **generation_args)

    proofs = [result['generated_text'][len(prompt):].strip() for result in out]
    return proofs

from pathlib import Path
import gc
import torch
from tqdm import tqdm
from transformers import pipeline

def _sanitize_dir_name(name: str) -> str:
    return str(name).replace("/", "_").replace("\\", "_")

def _next_index(out_dir: Path) -> int:
    out_dir.mkdir(parents=True, exist_ok=True)
    nums = []
    for p in out_dir.glob("*.txt"):
        stem = p.stem
        if stem.isdigit():
            nums.append(int(stem))
    return (max(nums) + 1) if nums else 1

model_ref = None
tok_ref = None
pipe_ref = None
def generate_proofs_memory_safe(
    model_ids,
    problem_row,
    problem_key,                 # e.g., DataFrame index or a unique ID column
    max_attempts: int,
    base_output_dir: str, # Added base_output_dir
    gpu_batch_size: int = 8,
    clear = True
):
    """
    Generate proofs and write to base_output_dir/<problem_key>/<model_id>/1.txt, 2.txt, ...
    No proof checking; purely generation + IO. Memory-safe (loads one model at a time).
    """
    for model_id in tqdm(model_ids, desc="Models"):
        model = tok = pipe = None
        attempt_bar = None
        try:
            model, tok = _load_model(model_id)
            pipe = pipeline("text-generation", model=model, tokenizer=tok, device=0)

            attempts_left = max_attempts
            attempt_bar = tqdm(total=max_attempts, desc=f"Generating {model_id}", leave=False)

            # Prepare output directory and next index (continues numbering if rerun)
            out_dir = Path(base_output_dir) / _sanitize_dir_name(problem_key) / _sanitize_dir_name(model_id) # Modified out_dir
            next_idx = _next_index(out_dir)

            print(next_idx)

            # raise Exception("Stop here")

            while attempts_left > 0:
                current_batch_size = min(gpu_batch_size, attempts_left)
                with torch.no_grad():
                    proof_snippets = generate_proof(
                        pipe,
                        header=problem_row['header'],
                        informal_prefix=problem_row['informal_prefix'],
                        formal_statement=problem_row['formal_statement'],
                        num_return_sequences=current_batch_size
                    )

                # Write each snippet to numbered files 1.txt, 2.txt, ...
                for snippet in proof_snippets:
                    out_path = out_dir / f"{next_idx}.txt"
                    with open(out_path, "w", encoding="utf-8") as f:
                        f.write(snippet)
                    next_idx += 1

                attempts_left -= current_batch_size
                attempt_bar.update(current_batch_size)

        finally:
          model_ref = model
          pipe_ref = pipe
          tok_ref = tok        
          # if clear:
          #   if attempt_bar is not None:
          #       attempt_bar.close()
          #   if model: del model
          #   if tok:   del tok
          #   if pipe:  del pipe
          #   gc.collect()
          #   torch.cuda.empty_cache()

def write_proofs_for_model(
    model_id: str,
    dataframe,
    base_output_dir: str, # Added base_output_dir
    max_attempts: int = 8,
    gpu_batch_size: int = 8,
    clear = True
):
    """
    For each problem in `dataframe`, generate `max_attempts` proofs for `model_id`
    and write them to base_output_dir/<problem_key>/<model_id>/*.txt.
    `problem_key` defaults to the DataFrame index value.
    """
    print(f"--- Generating proofs for model: {model_id} ---")
    for idx, problem_row in tqdm(dataframe.iterrows(), total=len(dataframe), desc=f"Problems for {model_id}"):
        problem_key = problem_row.get('problem_id', idx)  # prefer a column named 'problem_id' if present
        generate_proofs_memory_safe(
            model_ids=[model_id],
            problem_row=problem_row,
            problem_key=problem_key,
            max_attempts=max_attempts,
            base_output_dir=base_output_dir, # Passed base_output_dir
            gpu_batch_size=gpu_batch_size,
            clear = clear
        )

    if clear:
        # if attempt_bar is not None:
        #     attempt_bar.close()
        if model_ref: del model_ref
        if tok_ref:   del tok_ref
        if pipe_ref:  del pipe_ref
        gc.collect()
        torch.cuda.empty_cache()

    print(f"--- Done for {model_id}. ---")

!pip install sentencepiece

from datasets import load_dataset
# from google.colab import files
# import time

miniF2F_test_df = load_dataset("HaimingW/miniF2F-lean4", split="test").to_pandas().iloc[9:40]

for i, mid in enumerate(solver_model_ids):
    try:
      write_proofs_for_model(mid, miniF2F_test_df, base_output_dir=folder_path, max_attempts=16, gpu_batch_size=4, clear=False) # Passed folder_path
    except Exception as e:
      print(f"Error for {mid}: {e}")



!zip -r proofs.zip proofs
